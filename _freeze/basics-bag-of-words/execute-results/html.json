{
  "hash": "a4ab7b4ac7b5367909c3d13d2f4ffc50",
  "result": {
    "markdown": "---\ntitle: \"Basics\"\n---\n\n\n## Bag of Words\n\nThe \"bag of words\" model is the most common representation of text. We represent each document by counting how many times each word appears in it. It's simple, perhaps too simple.\n\nFor example, consider the following excerpt from the first State of the Union address by George Washington:\n\nFIGURE OUT HOW TO MAKE THE TEXT RENDER WRAPPED!\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-wrap}\nlibrary(tidyverse)\n\ntxts <- sotu::sotu_text\n\ntxts[[1]] |> \n  str_trunc(width = 500) |> \n  cat()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFellow-Citizens of the Senate and House of Representatives: \n\nI embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union,...\n```\n:::\n:::\n\n\nThis text has the following properties:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## number of characters\nnchar(txts[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6712\n```\n:::\n\n```{.r .cell-code}\n## number of words\nstr_count(txts[[1]], pattern = \"\\\\w+\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1091\n```\n:::\n:::\n\n\nBut turning this into a \"bag of words\" representation is not straight-forward. We must do some pre-processing first.\n\n### Pre-processing:\n\n@denny2018 describe 7 pre-processing choices that can affect downstream analyses.\n\n-   **Punctuation**.\n\n    Researchers must decide what types of characters represent valid text. We can be inclusive and choose to include all text, including numbers, html tags, punctuation, special characters (\\$, %, &, etc.), and extra-white space characters. Depending on the application, these characters may be informative (e.g., predicting authorship). In general, however, we remove punctuation.\n\n-   **Numbers**.\n\n-   **Lowercasing**.\n\n-   **Equivalence Classes (Stemming** **or Lemmatization).**\n\n    > Stemming refers to the process of reducing a word to its most basic form (Porter 1980). For example the words \"party\", \"partying\", and \"parties\" all share a common stem \"parti\". Stemming is often employed as a vocabulary reduction technique, as it combines different forms of a word together. However, stemming can sometimes combine together words with substantively different meanings (\"college students partying\", and \"political parties\"), which might be misleading in practice.\n    >\n    > @denny2018 [pp. 4]\n\n    Stemming might reduce two very different words to a common stem (e.g., \"secure\" and \"securities\"). *You might not want to use stemming for some tasks [@schofield2016].*\n\n    A more sophisticated alternative to stemming is called **lemmatization**, which maps tokens to their \"canonical form\" (such as one might find in a dictionary).\n\n-   **Stopword Removal**.\n\n    *Note that you might not want to remove stopwords for some tasks (e.g., predicting authorship). In fact, for some tasks you might want to remove all words except stopwords.*\n\n-   **n-gram Inclusion**.\n\n    The meaning of some words is greatly enhanced by adding just a little context; e.g., when talking about \"national defense\" or \"national debt.\" This leads some researchers to extract *n*-*grams* or sequences of tokens of length *n.* Adding n-grams to a document-term matrix can does be a good idea. But this may lead to an explosion in the size of the vocabulary.\n\n-   **Infrequently Used Terms**.\n\n    > A commonly used rule of thumb is to discard terms that appear in less than 0.5%--1% of documents (Grimmer 2010; Yano, Smith, and Wilkerson 2012; Grimmer and Stewart 2013); however, there has been no systematic study of the effects this preprocessing choice has on downstream analyses.\n    >\n    > @denny2018 [pp. 4-5]\n\n    We might want to reduce the size of the vocabulary for computational efficiency reasons or because we believe that very infrequently used terms will not contribute much information about document similarity.\n\n### Code\n\nThe `txts` object is a character vector of length 240 (one entry per speech).\n\nHere is what we do:\n\n1.  Tokenize\n2.  Reduce Complexity (with pre-processing)\n3.  Create a Document-Feature Matrix (or Document-Term Matrix).\n\nUsing the `quanteda` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.3.0\nUnicode version: 14.0\nICU version: 70.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 16 of 16 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n```{.r .cell-code}\nbow <- tokens(\n  x = txts,\n  what = \"word\",\n  remove_punct = TRUE, \n  split_hyphens = TRUE, \n  remove_numbers = TRUE\n) \n\nbow <- bow |> \n  tokens_tolower() |> \n  tokens_remove(stopwords(language = \"en\")) |> \n  tokens_wordstem()\n\nbow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 240 documents.\ntext1 :\n [1] \"fellow\"    \"citizen\"   \"senat\"     \"hous\"      \"repres\"    \"embrac\"   \n [7] \"great\"     \"satisfact\" \"opportun\"  \"now\"       \"present\"   \"congratul\"\n[ ... and 494 more ]\n\ntext2 :\n [1] \"fellow\"    \"citizen\"   \"senat\"     \"hous\"      \"repres\"    \"meet\"     \n [7] \"feel\"      \"much\"      \"satisfact\" \"abl\"       \"repeat\"    \"congratul\"\n[ ... and 622 more ]\n\ntext3 :\n [1] \"fellow\"   \"citizen\"  \"senat\"    \"hous\"     \"repres\"   \"vain\"    \n [7] \"may\"      \"expect\"   \"peac\"     \"indian\"   \"frontier\" \"long\"    \n[ ... and 1,064 more ]\n\ntext4 :\n [1] \"fellow\"    \"citizen\"   \"senat\"     \"hous\"      \"repres\"    \"abat\"     \n [7] \"satisfact\" \"meet\"      \"present\"   \"occas\"     \"felicit\"   \"continu\"  \n[ ... and 951 more ]\n\ntext5 :\n [1] \"fellow\"  \"citizen\" \"senat\"   \"hous\"    \"repres\"  \"sinc\"    \"commenc\"\n [8] \"term\"    \"call\"    \"offic\"   \"fit\"     \"occas\"  \n[ ... and 895 more ]\n\ntext6 :\n [1] \"fellow\"   \"citizen\"  \"senat\"    \"hous\"     \"repres\"   \"call\"    \n [7] \"mind\"     \"gracious\" \"indulg\"   \"heaven\"   \"american\" \"peopl\"   \n[ ... and 1,327 more ]\n\n[ reached max_ndoc ... 234 more documents ]\n```\n:::\n\n```{.r .cell-code}\ndfm1 <- quanteda::dfm(bow)\n\ndfm1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 240 documents, 14,411 features (91.34% sparse) and 0 docvars.\n       features\ndocs    fellow citizen senat hous repres embrac great satisfact opportun now\n  text1      2       3     2    3      3      1     4         2        1   1\n  text2      3       5     2    3      3      0     4         1        0   2\n  text3      1       3     3    3      3      1     0         3        1   0\n  text4      1       7     2    3      4      1     0         3        0   2\n  text5      2       5     2    3      3      1     0         0        1   2\n  text6      3      11     2    5      4      1     1         0        1   3\n[ reached max_ndoc ... 234 more documents, reached max_nfeat ... 14,401 more features ]\n```\n:::\n:::\n\n\nUsing the `tidytext` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\n\ndf_bow <- tibble::enframe(txts, name = \"id\", value = \"txt\") |> \n  ## this function makes a lot of the pre-processing decisions\n  ## less explicit, be careful.\n  unnest_tokens(\"word\", input = txt, to_lower = TRUE)\n\ndf_bow <- df_bow |> \n  anti_join(tidytext::get_stopwords()) |> \n  mutate(word = SnowballC::wordStem(word)) |> \n  count(id, word)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\ndfm2 <- df_bow |> \n  cast_dfm(document = id, term = word, value = n)\n\ndfm2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 240 documents, 19,908 features (93.40% sparse) and 0 docvars.\n    features\ndocs abroad access accord add adequ administr admit adopt advanc affair\n   1      1      1      1   1     1         1     1     1      1      3\n   2      1      0      0   2     0         1     0     0      0      1\n   3      0      0      0   0     2         1     0     1      2      1\n   4      2      0      2   3     1         0     0     1      0      1\n   5      0      0      2   0     0         0     0     1      1      1\n   6      0      0      1   0     1         0     0     1      2      1\n[ reached max_ndoc ... 234 more documents, reached max_nfeat ... 19,898 more features ]\n```\n:::\n:::\n\n\nSometimes it's preferable to work with the matrix object itself, for which it's recommended to use the sparse matrix objects (e.g., `dgCMatrix`) developed in the `Matrix` package.\n\nWe can then use simple operations to get stuff like sparsity (the percentage of empty cells in a matrix or the column sums (e.g., the total number of word counts across all documents).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Matrix)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Matrix'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n\n```{.r .cell-code}\nM1 <- df_bow |> \n  tidytext::cast_sparse(id, word, n)\n\nM1[1:15, 1:7]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n15 x 7 sparse Matrix of class \"dgCMatrix\"\n   abroad access accord add adequ administr admit\n1       1      1      1   1     1         1     1\n2       1      .      .   2     .         1     .\n3       .      .      .   .     2         1     .\n4       2      .      2   3     1         .     .\n5       .      .      2   .     .         .     .\n6       .      .      1   .     1         .     .\n7       .      .      1   2     2         .     .\n8       .      .      .   1     1         1     1\n9       .      .      .   1     1         .     .\n10      .      .      .   1     .         .     .\n11      .      .      .   .     .         1     .\n12      .      .      .   .     .         1     .\n13      1      .      .   1     .         2     2\n14      4      .      .   1     1         .     .\n15      .      .      1   .     .         .     .\n```\n:::\n\n```{.r .cell-code}\nmean(M1 == 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9340255\n```\n:::\n\n```{.r .cell-code}\ncolSums(M1) |> \n  sort(decreasing = TRUE) |> \n  head(n = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   state   govern     year   nation congress     unit      can  countri \n    9483     8746     7435     6480     5827     5381     4839     4774 \n   peopl     upon american      law     time      new     must \n    4517     4228     4003     3848     3717     3615     3600 \n```\n:::\n\n```{.r .cell-code}\nM2 <- as(dfm1, Class = \"sparseMatrix\")\n\nM1[1:15, 1:7]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n15 x 7 sparse Matrix of class \"dgCMatrix\"\n   abroad access accord add adequ administr admit\n1       1      1      1   1     1         1     1\n2       1      .      .   2     .         1     .\n3       .      .      .   .     2         1     .\n4       2      .      2   3     1         .     .\n5       .      .      2   .     .         .     .\n6       .      .      1   .     1         .     .\n7       .      .      1   2     2         .     .\n8       .      .      .   1     1         1     1\n9       .      .      .   1     1         .     .\n10      .      .      .   1     .         .     .\n11      .      .      .   .     .         1     .\n12      .      .      .   .     .         1     .\n13      1      .      .   1     .         2     2\n14      4      .      .   1     1         .     .\n15      .      .      1   .     .         .     .\n```\n:::\n\n```{.r .cell-code}\ndim(M1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   240 19908\n```\n:::\n\n```{.r .cell-code}\ndim(M2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   240 14411\n```\n:::\n\n```{.r .cell-code}\nM2[1:15, 1:7]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n15 x 7 sparse Matrix of class \"dgCMatrix\"\n        features\ndocs     fellow citizen senat hous repres embrac great\n  text1       2       3     2    3      3      1     4\n  text2       3       5     2    3      3      .     4\n  text3       1       3     3    3      3      1     .\n  text4       1       7     2    3      4      1     .\n  text5       2       5     2    3      3      1     .\n  text6       3      11     2    5      4      1     1\n  text7       2       4     4    4      5      .     1\n  text8       1       6     2    4      4      .     9\n  text9       1       8     2    4      3      .     5\n  text10      1       2     2    3      3      .     6\n  text11      1       3     3    4      3      .     4\n  text12      .       1     2    3      4      .     7\n  text13      3      10     1    1      1      .     6\n  text14      4       6     1    1      1      .     5\n  text15      2       7     3    3      2      .     2\n```\n:::\n:::\n\n\n## \n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- convert(dfm1, to = \"data.frame\")\n\nd <- d |> \n  pivot_longer(!doc_id, names_to = \"word\", values_to = \"n\") |> \n  filter(n > 0)\n\nd <- d |> \n  mutate(doc_id = readr::parse_number(doc_id))\n\nd <- d |> rename(id = doc_id) |> mutate(id = as.integer(id))\n\nd |> filter(id == 1) |> arrange(desc(n), word)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 342 × 3\n      id word        n\n   <int> <chr>   <dbl>\n 1     1 state       6\n 2     1 import      5\n 3     1 may         5\n 4     1 measur      5\n 5     1 public      5\n 6     1 respect     5\n 7     1 unit        5\n 8     1 countri     4\n 9     1 end         4\n10     1 govern      4\n# ℹ 332 more rows\n```\n:::\n\n```{.r .cell-code}\ndf_bow |> filter(id == 1) |> arrange(desc(n), word)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 342 × 3\n      id word        n\n   <int> <chr>   <int>\n 1     1 state       6\n 2     1 import      5\n 3     1 mai         5\n 4     1 measur      5\n 5     1 public      5\n 6     1 respect     5\n 7     1 unit        5\n 8     1 countri     4\n 9     1 end         4\n10     1 govern      4\n# ℹ 332 more rows\n```\n:::\n:::\n\n\n**Exercise:**\n\n*Use regular expressions to go from texts to document term matrix.*\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}