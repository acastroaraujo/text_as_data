[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text as Data",
    "section": "",
    "text": "Preface\nThis notebook is structured in the same way as Text as Data (2022): discovery, measurement, prediction, and causal inference. These are slightly different tasks and require their own forms of validation.\nThe authors share an agnostic approach to text-as-data (cf. Aronow and Miller 2019): there is no underlying “true” data-generating process out there. And even if there is, we won’t ever be able to adequately capture it.\n\nWe generally reject the view that there is an underlying structure that statistical models applied to text are recovering. Rather, we view statistical models as useful (and incomplete) summaries of the text documents.\nGrimmer, Roberts, and Stewart (2022, 11)\n\nWe will return to the issue of validation many times throughout this notebook. But it’s important to note that different disciplines think differently about this issue. In particular, computer scientists trust humans more than social scientists do (DiMaggio 2015). Much work in computer science and artificial intelligence has tried to replicate the results of human problem solving, often described as the “gold standard” against which the output of algorithms should be evaluated. In contrast, social scientists tend to be suspicious of human judgement, riddled with all sorts of biases, prejudices, and ideological priors.\nBy contrast, social scientists, at least those who have paid attention to work in cognitive psychology, are deeply suspicious of human judgment. The irony is that each group is most skeptical about the entities (algorithms or people) with which its expertise is most closely associated.\n\nComputer scientists who treat human judgments as a gold standard and social scientists who see in sentiment-analysis programs an antidote to human imperfection will often be disappointed, as algorithms and humans seem to be bad at pretty much the same types of tasks. For now at least, we can savor the irony that each group is most skeptical about the entities (algorithms or people) with which its expertise is most closely associated.\nDiMaggio (2015, 3)\n\n\n\n\n\nAronow, Peter M., and Benjamin T. Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.\n\n\nDiMaggio, Paul. 2015. “Adapting Computational Text Analysis to Social Science (and Vice Versa).” Big Data & Society 2 (2): 2053951715602908.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. Text as Data. Princeton University Press."
  },
  {
    "objectID": "intro.html#regular-expressions",
    "href": "intro.html#regular-expressions",
    "title": "1  Introduction",
    "section": "1.1 Regular Expressions",
    "text": "1.1 Regular Expressions\nRegular expressions are a language for specifying search within strings of text.\n\n“.”: matches any character.\n“\\” (or escape): escapes the special behaviour of expressions like “.”. For example, “\\.” will match “.” instead of any character.\nAnchors are used to match the start or end of a string.\n\n“^”: matches the start of the string.\n“$”: matches the end of the string.\n\n“\\d”: matches any digit.\n“\\s”: matches any white space (e.g. space, tab, newline)\n“[abc]”: matches a, b, or c. For example, “[Cc]olor” matches either “color” or “Color”.\nIn cases where there’s a well-defined sequence associated with a set of characters, these brackets can be mixed the “-” (dash) symbol to specify a range of characters; for example, “[abcdefghijk]” is equivalent to “[a-k]”.\n“[^abc]”: matches anything except a, b, or c.\n“|” (or disjunction operator): pick between one or more alternate patterns. To make the disjunction operator apply only to a specific pattern, we need to use the parenthesis operators “()”. For example, “pupp(y|ies)” matches either “puppy” or “puppies” (the disjunction only applies to the suffixes).\nRepetition:\n\n“*” (or Kleene star): matches zero or more instances of the previous pattern.\n“+”: matches one or more instance.\n“?”: matches zero or one instance.\nA range of numbers can also be specified:\n{n} \\(n\\) occurrences of the previous expression\n{n,m} from \\(n\\) to \\(m\\) occurrences of the previous expression\n{n,} at least \\(n\\) occurrences of the previous expression\n{,m} up to \\(m\\) occurrences of the previous expression\n\n\nExplore regular expressions using online tools like regex testers or regex puzzles.\nThis language will facilitate many real-world tasks, such as:\n\nTo determine which strings match a pattern\nTo find the positions of matches.\nTo extract the content of matches.\nTo replace matches with new values.\nTo split a string based on a match.\nEtc.\n\nWhen writing useful regular expressions, we will most likely be trying to minimize type I (false positive) and type II (false negative) errors. Keep in mind that there’s a trade-off between accuracy (or precision) and coverage (or recall).\nRegular Expression Substitution, Capture Groups,\nAn important use of regular expressions is in substitutions. For example, can use str_replace_all() from stringr or gsub() from base R to put angle brackets around all integers in a string:\n\nstring <- \"the 35 boxes were opened 12 times\"\nstr_replace_all(string, pattern = \"(\\\\d+)\", replacement = \"<\\\\1>\")\n\n[1] \"the <35> boxes were opened <12> times\"\n\ngsub(pattern = \"(\\\\d+)\", replacement = \"<\\\\1>\", string)\n\n[1] \"the <35> boxes were opened <12> times\"\n\n\nHere the “\\1” will be replaced by whatever string matched the first item in parentheses.\nThis use of parentheses to store a pattern in memory is called a capture group. Every time a capture group is used (i.e., parentheses surround a pattern), the resulting match is stored in a numbered register. If you match two different sets of parentheses, “\\2” means whatever matched the second capture group. Parentheses thus have a double function in regular expressions; they are used to group terms for specifying the order in which operators should apply, and they are used to capture something in a register.\nlook forwards and backwards\n(?=)\nfile:///Users/acastroaraujo/Documents/Notes/Other/Text.html#regular_expressions"
  },
  {
    "objectID": "basics-bow.html#bag-of-words",
    "href": "basics-bow.html#bag-of-words",
    "title": "Representation",
    "section": "Bag of Words",
    "text": "Bag of Words\nThe “bag of words” model is the most common representation of text. We represent each document by counting how many times each word appears in it. It’s simple, perhaps too simple.\nFor example, consider the following excerpt from the first State of the Union address by George Washington:\nFIGURE OUT HOW TO MAKE THE TEXT RENDER WRAPPED!\n\nlibrary(tidyverse)\n\ntxts <- sotu::sotu_text\n\ntxts[[1]] |> \n  str_trunc(width = 500) |> \n  cat()\n\nFellow-Citizens of the Senate and House of Representatives: \n\nI embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union,...\n\n\nThis text has the following properties:\n\n## number of characters\nnchar(txts[[1]])\n\n[1] 6712\n\n## number of words\nstr_count(txts[[1]], pattern = \"\\\\w+\")\n\n[1] 1091\n\n\nBut turning this into a “bag of words” representation is not straight-forward. We must do some pre-processing first.\n\nPre-processing:\nDenny and Spirling (2018) describe 7 pre-processing choices that can affect downstream analyses.\n\nPunctuation.\nResearchers must decide what types of characters represent valid text. We can be inclusive and choose to include all text, including numbers, html tags, punctuation, special characters ($, %, &, etc.), and extra-white space characters. Depending on the application, these characters may be informative (e.g., predicting authorship). In general, however, we remove punctuation.\nNumbers.\nLowercasing.\nEquivalence Classes (Stemming or Lemmatization).\n\nStemming refers to the process of reducing a word to its most basic form (Porter 1980). For example the words “party”, “partying”, and “parties” all share a common stem “parti”. Stemming is often employed as a vocabulary reduction technique, as it combines different forms of a word together. However, stemming can sometimes combine together words with substantively different meanings (“college students partying”, and “political parties”), which might be misleading in practice.\nDenny and Spirling (2018, 4)\n\nStemming might reduce two very different words to a common stem (e.g., “secure” and “securities”). You might not want to use stemming for some tasks (Schofield and Mimno 2016).\nA more sophisticated alternative to stemming is called lemmatization, which maps tokens to their “canonical form” (such as one might find in a dictionary).\nStopword Removal.\nNote that you might not want to remove stopwords for some tasks (e.g., predicting authorship). In fact, for some tasks you might want to remove all words except stopwords.\nn-gram Inclusion.\nThe meaning of some words is greatly enhanced by adding just a little context; e.g., when talking about “national defense” or “national debt.” This leads some researchers to extract n-grams or sequences of tokens of length n. Adding n-grams to a document-term matrix can does be a good idea. But this may lead to an explosion in the size of the vocabulary.\nInfrequently Used Terms.\n\nA commonly used rule of thumb is to discard terms that appear in less than 0.5%–1% of documents (Grimmer 2010; Yano, Smith, and Wilkerson 2012; Grimmer and Stewart 2013); however, there has been no systematic study of the effects this preprocessing choice has on downstream analyses.\nDenny and Spirling (2018, 4–5)\n\nWe might want to reduce the size of the vocabulary for computational efficiency reasons or because we believe that very infrequently used terms will not contribute much information about document similarity.\n\n\n\nCode\nThe txts object is a character vector of length 240 (one entry per speech).\nHere is what we do:\n\nTokenize\nReduce Complexity (with pre-processing)\nCreate a Document-Feature Matrix (or Document-Term Matrix).\n\nUsing the quanteda package:\n\nlibrary(quanteda)\n\nPackage version: 3.3.0\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 16 of 16 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nbow <- tokens(\n  x = txts,\n  what = \"word\",\n  remove_punct = TRUE, \n  split_hyphens = TRUE, \n  remove_numbers = TRUE\n) \n\nbow <- bow |> \n  tokens_tolower() |> \n  tokens_remove(stopwords(language = \"en\")) |> \n  tokens_wordstem()\n\nbow\n\nTokens consisting of 240 documents.\ntext1 :\n [1] \"fellow\"    \"citizen\"   \"senat\"     \"hous\"      \"repres\"    \"embrac\"   \n [7] \"great\"     \"satisfact\" \"opportun\"  \"now\"       \"present\"   \"congratul\"\n[ ... and 494 more ]\n\ntext2 :\n [1] \"fellow\"    \"citizen\"   \"senat\"     \"hous\"      \"repres\"    \"meet\"     \n [7] \"feel\"      \"much\"      \"satisfact\" \"abl\"       \"repeat\"    \"congratul\"\n[ ... and 622 more ]\n\ntext3 :\n [1] \"fellow\"   \"citizen\"  \"senat\"    \"hous\"     \"repres\"   \"vain\"    \n [7] \"may\"      \"expect\"   \"peac\"     \"indian\"   \"frontier\" \"long\"    \n[ ... and 1,064 more ]\n\ntext4 :\n [1] \"fellow\"    \"citizen\"   \"senat\"     \"hous\"      \"repres\"    \"abat\"     \n [7] \"satisfact\" \"meet\"      \"present\"   \"occas\"     \"felicit\"   \"continu\"  \n[ ... and 951 more ]\n\ntext5 :\n [1] \"fellow\"  \"citizen\" \"senat\"   \"hous\"    \"repres\"  \"sinc\"    \"commenc\"\n [8] \"term\"    \"call\"    \"offic\"   \"fit\"     \"occas\"  \n[ ... and 895 more ]\n\ntext6 :\n [1] \"fellow\"   \"citizen\"  \"senat\"    \"hous\"     \"repres\"   \"call\"    \n [7] \"mind\"     \"gracious\" \"indulg\"   \"heaven\"   \"american\" \"peopl\"   \n[ ... and 1,327 more ]\n\n[ reached max_ndoc ... 234 more documents ]\n\ndfm1 <- quanteda::dfm(bow)\n\ndfm1\n\nDocument-feature matrix of: 240 documents, 14,411 features (91.34% sparse) and 0 docvars.\n       features\ndocs    fellow citizen senat hous repres embrac great satisfact opportun now\n  text1      2       3     2    3      3      1     4         2        1   1\n  text2      3       5     2    3      3      0     4         1        0   2\n  text3      1       3     3    3      3      1     0         3        1   0\n  text4      1       7     2    3      4      1     0         3        0   2\n  text5      2       5     2    3      3      1     0         0        1   2\n  text6      3      11     2    5      4      1     1         0        1   3\n[ reached max_ndoc ... 234 more documents, reached max_nfeat ... 14,401 more features ]\n\n\nUsing the tidytext package:\n\nlibrary(tidytext)\n\ndf_bow <- tibble::enframe(txts, name = \"id\", value = \"txt\") |> \n  ## this function makes a lot of the pre-processing decisions\n  ## less explicit, be careful.\n  unnest_tokens(\"word\", input = txt, to_lower = TRUE)\n\ndf_bow <- df_bow |> \n  anti_join(tidytext::get_stopwords()) |> \n  mutate(word = SnowballC::wordStem(word)) |> \n  count(id, word)\n\nJoining with `by = join_by(word)`\n\ndfm2 <- df_bow |> \n  cast_dfm(document = id, term = word, value = n)\n\ndfm2\n\nDocument-feature matrix of: 240 documents, 19,908 features (93.40% sparse) and 0 docvars.\n    features\ndocs abroad access accord add adequ administr admit adopt advanc affair\n   1      1      1      1   1     1         1     1     1      1      3\n   2      1      0      0   2     0         1     0     0      0      1\n   3      0      0      0   0     2         1     0     1      2      1\n   4      2      0      2   3     1         0     0     1      0      1\n   5      0      0      2   0     0         0     0     1      1      1\n   6      0      0      1   0     1         0     0     1      2      1\n[ reached max_ndoc ... 234 more documents, reached max_nfeat ... 19,898 more features ]\n\n\nSometimes it’s preferable to work with the matrix object itself, for which it’s recommended to use the sparse matrix objects (e.g., dgCMatrix) developed in the Matrix package.\nWe can then use simple operations to get stuff like sparsity (the percentage of empty cells in a matrix or the column sums (e.g., the total number of word counts across all documents).\n\nlibrary(Matrix)\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nM1 <- df_bow |> \n  tidytext::cast_sparse(id, word, n)\n\nM1[1:15, 1:7]\n\n15 x 7 sparse Matrix of class \"dgCMatrix\"\n   abroad access accord add adequ administr admit\n1       1      1      1   1     1         1     1\n2       1      .      .   2     .         1     .\n3       .      .      .   .     2         1     .\n4       2      .      2   3     1         .     .\n5       .      .      2   .     .         .     .\n6       .      .      1   .     1         .     .\n7       .      .      1   2     2         .     .\n8       .      .      .   1     1         1     1\n9       .      .      .   1     1         .     .\n10      .      .      .   1     .         .     .\n11      .      .      .   .     .         1     .\n12      .      .      .   .     .         1     .\n13      1      .      .   1     .         2     2\n14      4      .      .   1     1         .     .\n15      .      .      1   .     .         .     .\n\nmean(M1 == 0)\n\n[1] 0.9340255\n\ncolSums(M1) |> \n  sort(decreasing = TRUE) |> \n  head(n = 15)\n\n   state   govern     year   nation congress     unit      can  countri \n    9483     8746     7435     6480     5827     5381     4839     4774 \n   peopl     upon american      law     time      new     must \n    4517     4228     4003     3848     3717     3615     3600 \n\nM2 <- as(dfm1, Class = \"sparseMatrix\")\n\nM1[1:15, 1:7]\n\n15 x 7 sparse Matrix of class \"dgCMatrix\"\n   abroad access accord add adequ administr admit\n1       1      1      1   1     1         1     1\n2       1      .      .   2     .         1     .\n3       .      .      .   .     2         1     .\n4       2      .      2   3     1         .     .\n5       .      .      2   .     .         .     .\n6       .      .      1   .     1         .     .\n7       .      .      1   2     2         .     .\n8       .      .      .   1     1         1     1\n9       .      .      .   1     1         .     .\n10      .      .      .   1     .         .     .\n11      .      .      .   .     .         1     .\n12      .      .      .   .     .         1     .\n13      1      .      .   1     .         2     2\n14      4      .      .   1     1         .     .\n15      .      .      1   .     .         .     .\n\ndim(M1)\n\n[1]   240 19908\n\ndim(M2)\n\n[1]   240 14411\n\nM2[1:15, 1:7]\n\n15 x 7 sparse Matrix of class \"dgCMatrix\"\n        features\ndocs     fellow citizen senat hous repres embrac great\n  text1       2       3     2    3      3      1     4\n  text2       3       5     2    3      3      .     4\n  text3       1       3     3    3      3      1     .\n  text4       1       7     2    3      4      1     .\n  text5       2       5     2    3      3      1     .\n  text6       3      11     2    5      4      1     1\n  text7       2       4     4    4      5      .     1\n  text8       1       6     2    4      4      .     9\n  text9       1       8     2    4      3      .     5\n  text10      1       2     2    3      3      .     6\n  text11      1       3     3    4      3      .     4\n  text12      .       1     2    3      4      .     7\n  text13      3      10     1    1      1      .     6\n  text14      4       6     1    1      1      .     5\n  text15      2       7     3    3      2      .     2"
  },
  {
    "objectID": "basics-bow.html#section",
    "href": "basics-bow.html#section",
    "title": "Representation",
    "section": "",
    "text": "d <- convert(dfm1, to = \"data.frame\")\n\nd <- d |> \n  pivot_longer(!doc_id, names_to = \"word\", values_to = \"n\") |> \n  filter(n > 0)\n\nd <- d |> \n  mutate(doc_id = readr::parse_number(doc_id))\n\nd <- d |> rename(id = doc_id) |> mutate(id = as.integer(id))\n\nd |> filter(id == 1) |> arrange(desc(n), word)\n\n# A tibble: 342 × 3\n      id word        n\n   <int> <chr>   <dbl>\n 1     1 state       6\n 2     1 import      5\n 3     1 may         5\n 4     1 measur      5\n 5     1 public      5\n 6     1 respect     5\n 7     1 unit        5\n 8     1 countri     4\n 9     1 end         4\n10     1 govern      4\n# ℹ 332 more rows\n\ndf_bow |> filter(id == 1) |> arrange(desc(n), word)\n\n# A tibble: 342 × 3\n      id word        n\n   <int> <chr>   <int>\n 1     1 state       6\n 2     1 import      5\n 3     1 mai         5\n 4     1 measur      5\n 5     1 public      5\n 6     1 respect     5\n 7     1 unit        5\n 8     1 countri     4\n 9     1 end         4\n10     1 govern      4\n# ℹ 332 more rows\n\n\nExercise:\nUse regular expressions to go from texts to document term matrix.\n\n\n\n\nDenny, Matthew J., and Arthur Spirling. 2018. “Text Preprocessing for Unsupervised Learning: Why It Matters, When It Misleads, and What to Do about It.” Political Analysis 26 (2): 168189.\n\n\nSchofield, Alexandra, and David Mimno. 2016. “Comparing Apples to Apple: The Effects of Stemmers on Topic Models.” Transactions of the Association for Computational Linguistics 4 (July): 287–300. https://doi.org/10.1162/tacl_a_00099."
  },
  {
    "objectID": "basics-word-embeddings.html",
    "href": "basics-word-embeddings.html",
    "title": "4  Word Embeddings",
    "section": "",
    "text": "You shall know a word by the company it keeps.\n— John R. Firth\n\nSimilarity\nEncoder\nDecoder"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aronow, Peter M., and Benjamin T. Miller. 2019. Foundations of\nAgnostic Statistics. Cambridge University Press.\n\n\nDenny, Matthew J., and Arthur Spirling. 2018. “Text Preprocessing\nfor Unsupervised Learning: Why It Matters, When It Misleads, and What to\nDo about It.” Political Analysis 26 (2): 168189.\n\n\nDiMaggio, Paul. 2015. “Adapting Computational Text Analysis to\nSocial Science (and Vice Versa).” Big Data & Society\n2 (2): 2053951715602908.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022.\nText as Data. Princeton University Press.\n\n\nSchofield, Alexandra, and David Mimno. 2016. “Comparing Apples to\nApple: The Effects of Stemmers on Topic Models.” Transactions\nof the Association for Computational Linguistics 4 (July): 287–300.\nhttps://doi.org/10.1162/tacl_a_00099."
  }
]